<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning on tiany&#39;s Blog</title>
    <link>https://windytiany.github.io/tags/machine-learning/</link>
    <description>Recent content in machine learning on tiany&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>tiany - ShadowGarden</copyright>
    <lastBuildDate>Fri, 10 Feb 2023 18:55:03 +0800</lastBuildDate><atom:link href="https://windytiany.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Notes on machine learning 2</title>
      <link>https://windytiany.github.io/posts/machinelearningpartii/</link>
      <pubDate>Fri, 10 Feb 2023 18:55:03 +0800</pubDate>
      
      <guid>https://windytiany.github.io/posts/machinelearningpartii/</guid>
      <description>Diagram of a neural network model Implement a neural network Libraries:
import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.activations import relu, linear, sigmoid, softmax from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy from tensorflow.keras.optimizers import Adam Create a layer TensorFlow:
Dense(units=, activation=&amp;#39;&amp;#39;) DIY:
def my_dense_layer(a_in, W, b, g): &amp;#39;&amp;#39;&amp;#39; a_in: input W: parameters b: parameters g: activation function &amp;#39;&amp;#39;&amp;#39; units = W.shape[1] # number of neurons in this layer a_out = np.</description>
    </item>
    
  </channel>
</rss>
